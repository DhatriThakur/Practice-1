{
  "cells": [
    {
      "metadata": {
        "id": "wdj9RMfoGPC2"
      },
      "cell_type": "markdown",
      "source": [
        "Colab is making it easier than ever to integrate powerful Generative AI capabilities into your projects. We are launching public preview for a simple and intuitive Python library (google.colab.ai) to access state-of-the-art language models directly within Pro and Pro+ subscriber Colab environments.  This means subscribers can spend less time on configuration and set up and more time bringing their ideas to life. With just a few lines of code, you can now perform a variety of tasks:\n",
        "- Generate text\n",
        "- Translate languages\n",
        "- Write creative content\n",
        "- Categorize text\n",
        "\n",
        "Happy Coding!\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Ucchuu5vV3Jp",
        "outputId": "7fc314eb-f351-4d15-e553-ef349c7f125d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# @title List available models\n",
        "from google.colab import ai\n",
        "\n",
        "ai.list_models()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['google/gemini-2.5-flash', 'google/gemini-2.5-flash-lite']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "LjfCGEpzDsD9"
      },
      "cell_type": "markdown",
      "source": [
        "Choosing a Model\n",
        "The model names give you a hint about their capabilities and intended use:\n",
        "\n",
        "Pro: These are the most capable models, ideal for complex reasoning, creative tasks, and detailed analysis.\n",
        "\n",
        "Flash: These models are optimized for high speed and efficiency, making them great for summarization, chat applications, and tasks requiring rapid responses.\n",
        "\n",
        "Gemma: These are lightweight, open-weight models suitable for a variety of text generation tasks and are great for experimentation."
      ]
    },
    {
      "metadata": {
        "id": "R7taibpc7x2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "a33600f8-50d6-4760-cdcf-ede03206eec7"
      },
      "cell_type": "code",
      "source": [
        "# @title Simple batch generation example\n",
        "# Only text-to-text input/output is supported\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'message': 'Colab Models is only available to Colab Pro and Pro+ subscribers.', 'type': 'invalid_request_error'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1782392906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the capital of France?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/ai.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, model_name, stream)\u001b[0m\n\u001b[1;32m     83\u001b[0m   )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'message': 'Colab Models is only available to Colab Pro and Pro+ subscribers.', 'type': 'invalid_request_error'}"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "NHO9VzO9AHZP"
      },
      "cell_type": "code",
      "source": [
        "# @title Choose a different model\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of England\", model_name='google/gemini-2.0-flash-lite')\n",
        "print(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ysDdFbH_Dgtz"
      },
      "cell_type": "markdown",
      "source": [
        "For longer text generations, you can stream the response. This displays the output token by token as it's generated, rather than waiting for the entire response to complete. This provides a more interactive and responsive experience. To enable this, simply set stream=True."
      ]
    },
    {
      "metadata": {
        "id": "4BNgxiB6--_5"
      },
      "cell_type": "code",
      "source": [
        "# @title Simple streaming example\n",
        "from google.colab import ai\n",
        "\n",
        "stream = ai.generate_text(\"Tell me a short story.\", stream=True)\n",
        "for text in stream:\n",
        "  print(text, end='')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CpMmpaVClSBV"
      },
      "cell_type": "code",
      "source": [
        "#@title Text formatting setup\n",
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"…\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "003ba7e3"
      },
      "source": [
        "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.\n",
        "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel. Give it the name `GOOGLE_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38c02784"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c53dbdc"
      },
      "source": [
        "Before you can make any API calls, you need to initialize the Generative Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7814d03"
      },
      "source": [
        "# Initialize the Gemini API\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24093290"
      },
      "source": [
        "Now you can make API calls. For example, to generate a poem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c7ebf15"
      },
      "source": [
        "response = gemini_model.generate_content('Write a poem about the moon.')\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWiLPzTnRoy-"
      },
      "cell_type": "code",
      "source": [
        "# @title Formatted streaming example\n",
        "from google.colab import ai\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Give me a long winded description about the evolution of the Roman Empire.', model_name='google/gemini-2.0-flash', stream=True):\n",
        "  wrapper.print(chunk)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import sqrt\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- 1. Decision Tree Node and Classifier (Modified from previous assignment) ---\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Represents a single node in the Decision Tree.\"\"\"\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    \"\"\"Custom Decision Tree Classifier using Gini Impurity, now supporting feature subsetting.\"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, n_features_subset=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "        self.n_features_subset = n_features_subset\n",
        "        self.feature_importance = None # To store feature importance (Gini reduction)\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        \"\"\"Calculates the Gini Impurity of a set of labels y.\"\"\"\n",
        "        if len(y) == 0: return 0.0\n",
        "        m = len(y)\n",
        "        gini = 1.0\n",
        "        for count in Counter(y).values():\n",
        "            gini -= (count / m)**2\n",
        "        return gini\n",
        "\n",
        "    def _information_gain(self, y_parent, y_left, y_right):\n",
        "        \"\"\"Calculates Information Gain based on Gini impurity.\"\"\"\n",
        "        gini_parent = self._gini_impurity(y_parent)\n",
        "        n_parent = len(y_parent)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if n_parent == 0: return 0\n",
        "\n",
        "        weighted_impurity = (len(y_left) / n_parent) * self._gini_impurity(y_left) + \\\n",
        "                            (len(y_right) / n_parent) * self._gini_impurity(y_right)\n",
        "\n",
        "        return gini_parent - weighted_impurity\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Finds the feature and threshold that yield the highest Information Gain.\"\"\"\n",
        "        m, n = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Implement Feature Randomness: select a random subset of features\n",
        "        if self.n_features_subset is None or self.n_features_subset >= n:\n",
        "            features_to_check = range(n)\n",
        "        else:\n",
        "            # Randomly select self.n_features_subset indices without replacement\n",
        "            features_to_check = np.random.choice(n, self.n_features_subset, replace=False)\n",
        "\n",
        "        for feature_index in features_to_check:\n",
        "            X_column = X[:, feature_index]\n",
        "            # Consider all unique values in the feature column as potential thresholds\n",
        "            possible_thresholds = np.unique(X_column)\n",
        "\n",
        "            for threshold in possible_thresholds:\n",
        "                y_left = y[X_column <= threshold]\n",
        "                y_right = y[X_column > threshold]\n",
        "\n",
        "                if len(y_left) == 0 or len(y_right) == 0: continue\n",
        "\n",
        "                gain = self._information_gain(y, y_left, y_right)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature_index, best_threshold, best_gain\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"Returns the most frequent class label in the subset y.\"\"\"\n",
        "        return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "    def _update_feature_importance(self, feature_index, gain, X_size):\n",
        "        \"\"\"Updates feature importance based on the split's Gini reduction (used only for internal nodes).\"\"\"\n",
        "        if self.feature_importance is None:\n",
        "            self.feature_importance = np.zeros(X_size)\n",
        "\n",
        "        # Weight the gain by the proportion of samples at this node\n",
        "        self.feature_importance[feature_index] += gain * (X_size / self.X_train_size)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursive function to build the decision tree.\"\"\"\n",
        "        m, n = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "        # --- Stopping Criteria ---\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return Node(value=self._most_common_label(y))\n",
        "        if m < self.min_samples_split:\n",
        "            return Node(value=self._most_common_label(y))\n",
        "        if num_labels == 1:\n",
        "            return Node(value=y[0])\n",
        "\n",
        "        # --- Find the best split ---\n",
        "        feature_index, threshold, gain = self._best_split(X, y)\n",
        "\n",
        "        if gain <= 0:\n",
        "            return Node(value=self._most_common_label(y))\n",
        "\n",
        "        # Update feature importance for this tree (used only for Random Forest aggregation)\n",
        "        if self.X_train_size is not None:\n",
        "             self._update_feature_importance(feature_index, gain, m)\n",
        "\n",
        "        # --- Perform the split ---\n",
        "        X_column = X[:, feature_index]\n",
        "        left_indices = X_column <= threshold\n",
        "\n",
        "        X_left, y_left = X[left_indices], y[left_indices]\n",
        "        X_right, y_right = X[~left_indices], y[~left_indices]\n",
        "\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return Node(feature_index, threshold, left_child, right_child)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Initiates the recursive tree-building process.\"\"\"\n",
        "        # Store size for feature importance calculation weighting\n",
        "        self.X_train_size = len(X)\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Helper to traverse the tree for a single sample x.\"\"\"\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the class labels for a dataset X.\"\"\"\n",
        "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "# --- 2. Random Forest Classifier Implementation (Part A, Task 4) ---\n",
        "\n",
        "class RandomForestClassifier:\n",
        "    \"\"\"Ensemble of Decision Trees using Bagging and Feature Randomness.\"\"\"\n",
        "    def __init__(self, n_trees=50, max_depth=None, min_samples_split=2, n_features=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_features = n_features\n",
        "        self.trees = []\n",
        "        self.total_features = 0\n",
        "        self.oob_indices = [] # Bonus Challenge\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        \"\"\"Creates a bootstrap sample (sampling with replacement) and records OOB indices.\"\"\"\n",
        "        m = len(X)\n",
        "        # Select m indices with replacement\n",
        "        sample_indices = np.random.choice(m, m, replace=True)\n",
        "\n",
        "        # Determine Out-of-Bag (OOB) indices\n",
        "        all_indices = np.arange(m)\n",
        "        oob_indices = np.setdiff1d(all_indices, np.unique(sample_indices))\n",
        "        self.oob_indices.append(oob_indices) # Store for OOB error calculation\n",
        "\n",
        "        return X[sample_indices], y[sample_indices]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Trains n_trees Decision Trees using bagging and feature randomness.\"\"\"\n",
        "        self.total_features = X.shape[1]\n",
        "        self.trees = []\n",
        "\n",
        "        # Determine the number of features to use at each split (default: sqrt(total_features))\n",
        "        if self.n_features is None:\n",
        "            self.n_features = int(sqrt(self.total_features))\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            # 1. Create a bootstrap sample (Bagging)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "\n",
        "            # 2. Train a Decision Tree with Feature Randomness\n",
        "            tree = DecisionTreeClassifier(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                n_features_subset=self.n_features # Pass n_features to control randomness\n",
        "            )\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "        # Calculate combined feature importance after all trees are grown\n",
        "        self.feature_importances_ = self._calculate_feature_importance()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _calculate_feature_importance(self):\n",
        "        \"\"\"Aggregates feature importance (Gini reduction) across all trees.\"\"\"\n",
        "        total_importance = np.zeros(self.total_features)\n",
        "\n",
        "        for tree in self.trees:\n",
        "            if tree.feature_importance is not None:\n",
        "                total_importance += tree.feature_importance\n",
        "\n",
        "        # Normalize the importance scores\n",
        "        if np.sum(total_importance) > 0:\n",
        "            return total_importance / np.sum(total_importance)\n",
        "        return total_importance\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Aggregates predictions from all trees using majority vote.\"\"\"\n",
        "        # Get predictions from every tree\n",
        "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "\n",
        "        # Transpose to get (n_samples, n_trees)\n",
        "        tree_predictions = tree_predictions.T\n",
        "\n",
        "        final_predictions = np.array([\n",
        "            Counter(pred).most_common(1)[0][0]\n",
        "            for pred in tree_predictions\n",
        "        ])\n",
        "        return final_predictions\n",
        "\n",
        "    # --- Bonus Challenge: Out-of-Bag (OOB) Error Estimation ---\n",
        "    def oob_error(self, X_train, y_train):\n",
        "        \"\"\"Estimates generalization error using Out-of-Bag samples.\"\"\"\n",
        "        m = len(X_train)\n",
        "        oob_predictions = np.empty((m, self.n_trees)) * np.nan # NaN for samples not in OOB\n",
        "\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            # Get the indices that were NOT used for training this tree\n",
        "            oob_indices = self.oob_indices[i]\n",
        "\n",
        "            if len(oob_indices) > 0:\n",
        "                X_oob = X_train[oob_indices]\n",
        "                y_oob = y_train[oob_indices]\n",
        "\n",
        "                # Get predictions for the OOB samples\n",
        "                predictions = tree.predict(X_oob)\n",
        "\n",
        "                # Store predictions in the full matrix at the correct indices\n",
        "                for idx, pred in zip(oob_indices, predictions):\n",
        "                    oob_predictions[idx, i] = pred\n",
        "\n",
        "        # For each sample, find the majority vote among trees where it was OOB\n",
        "        final_oob_preds = []\n",
        "        for i in range(m):\n",
        "            # Predictions from trees where sample i was OOB (non-NaN values)\n",
        "            valid_preds = oob_predictions[i][~np.isnan(oob_predictions[i])]\n",
        "\n",
        "            if len(valid_preds) > 0:\n",
        "                # Majority vote for that sample\n",
        "                final_oob_preds.append(Counter(valid_preds).most_common(1)[0][0])\n",
        "            else:\n",
        "                # If a sample was in every bootstrap (rare), assign majority class of the full training set\n",
        "                final_oob_preds.append(Counter(y_train).most_common(1)[0][0])\n",
        "\n",
        "        # Calculate error\n",
        "        final_oob_preds = np.array(final_oob_preds)\n",
        "\n",
        "        # Compare OOB prediction with actual training label\n",
        "        return 1.0 - calculate_accuracy(y_train, final_oob_preds)\n",
        "\n",
        "# --- 3. Evaluation Metrics (Part B, Task 1) ---\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, labels=[0, 1]):\n",
        "    \"\"\"Calculates the confusion matrix components (TP, FP, FN, TN).\"\"\"\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    return TP, FP, FN, TN\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    \"\"\"Calculates classification accuracy.\"\"\"\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "def precision_score(y_true, y_pred):\n",
        "    \"\"\"Precision: TP / (TP + FP)\"\"\"\n",
        "    TP, FP, _, _ = confusion_matrix(y_true, y_pred)\n",
        "    # Handle division by zero\n",
        "    return TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "\n",
        "def recall_score(y_true, y_pred):\n",
        "    \"\"\"Recall: TP / (TP + FN)\"\"\"\n",
        "    TP, _, FN, _ = confusion_matrix(y_true, y_pred)\n",
        "    # Handle division by zero\n",
        "    return TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    \"\"\"F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\"\"\"\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    # Handle division by zero\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "# --- 4. Data Loading and Preprocessing (Part A, Task 2) ---\n",
        "\n",
        "def load_wine_data(url):\n",
        "    \"\"\"Loads the Wine Quality (Red) dataset and converts it to binary classification.\"\"\"\n",
        "    try:\n",
        "        # Use requests to fetch data, then pandas to read\n",
        "        s = requests.get(url).content\n",
        "        df = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=';')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from URL. Using local fallback (mock data) if available.\")\n",
        "        # Fallback needed if external connection fails\n",
        "        # Using a simple mock dataset if fetching fails for demonstration purposes\n",
        "        df = pd.DataFrame(np.random.rand(100, 12), columns=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality'])\n",
        "\n",
        "    # Features and target\n",
        "    X = df.drop('quality', axis=1).values\n",
        "    y_raw = df['quality'].values\n",
        "\n",
        "    # Binary Classification: quality > 5 is 'good' (1), <= 5 is 'bad' (0)\n",
        "    y_binary = (y_raw > 5).astype(int)\n",
        "\n",
        "    feature_names = df.drop('quality', axis=1).columns.to_list()\n",
        "\n",
        "    # Simple standardization for demonstration, though not strictly required for DT/RF,\n",
        "    # it is often good practice if the ensemble is later used for regression or different tasks.\n",
        "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "    return X, y_binary, feature_names\n",
        "\n",
        "WINE_URL_RED = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "X, y, feature_names = load_wine_data(WINE_URL_RED)\n",
        "\n",
        "# Split the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 5. Model Training and Comparison (Part B, Task 2) ---\n",
        "\n",
        "# --- A. Single Overfitting Decision Tree (Max Depth = high) ---\n",
        "single_tree = DecisionTreeClassifier(max_depth=100) # Ensure it grows deep\n",
        "single_tree.fit(X_train, y_train)\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "\n",
        "metrics_single = {\n",
        "    'Accuracy': calculate_accuracy(y_test, y_pred_single),\n",
        "    'Precision': precision_score(y_test, y_pred_single),\n",
        "    'Recall': recall_score(y_test, y_pred_single),\n",
        "    'F1-Score': f1_score(y_test, y_pred_single),\n",
        "}\n",
        "\n",
        "# --- B. Random Forest Classifier ---\n",
        "# Calculate the optimal n_features for this dataset (sqrt(11) approx 3)\n",
        "N_FEATURES_SUBSET = int(sqrt(X_train.shape[1]))\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_trees=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=2,\n",
        "    n_features=N_FEATURES_SUBSET # Using 3 features per split\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "metrics_rf = {\n",
        "    'Accuracy': calculate_accuracy(y_test, y_pred_rf),\n",
        "    'Precision': precision_score(y_test, y_pred_rf),\n",
        "    'Recall': recall_score(y_test, y_pred_rf),\n",
        "    'F1-Score': f1_score(y_test, y_pred_rf),\n",
        "}\n",
        "\n",
        "# --- C. Bonus Challenge: OOB Error ---\n",
        "oob_error_rate = rf_model.oob_error(X_train, y_train)\n",
        "test_error_rate = 1.0 - metrics_rf['Accuracy']\n",
        "\n",
        "# Print Comparison Table\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "print(\"Model Performance Comparison (Binary Wine Classification)\")\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "print(f\"{'Metric':<10} | {'Single Deep Tree':<20} | {'Random Forest (100 Trees)':<25}\")\n",
        "print(\"-\" * 60)\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
        "    print(f\"{metric:<10} | {metrics_single[metric]:<20.4f} | {metrics_rf[metric]:<25.4f}\")\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "print(f\"Bonus: OOB Error Estimate: {oob_error_rate:.4f} (Test Error: {test_error_rate:.4f})\")\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "# --- 6. Visualization: Accuracy vs. Number of Trees (Part B, Task 3) ---\n",
        "\n",
        "n_trees_range = [1, 5, 10, 25, 50, 100]\n",
        "rf_accuracy_over_trees = []\n",
        "\n",
        "print(\"\\nEvaluating Accuracy vs. Number of Trees...\")\n",
        "for n in n_trees_range:\n",
        "    # Use consistent depth and features for fair comparison\n",
        "    temp_rf = RandomForestClassifier(n_trees=n, max_depth=10, n_features=N_FEATURES_SUBSET)\n",
        "    temp_rf.fit(X_train, y_train)\n",
        "    y_pred_temp = temp_rf.predict(X_test)\n",
        "    rf_accuracy_over_trees.append(calculate_accuracy(y_test, y_pred_temp))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_trees_range, rf_accuracy_over_trees, marker='o', linestyle='-', color='darkgreen')\n",
        "plt.title('Random Forest Test Accuracy vs. Number of Trees')\n",
        "plt.xlabel('Number of Trees (n_trees)')\n",
        "plt.ylabel('Test Set Accuracy')\n",
        "plt.xticks(n_trees_range)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show() # [Image of Accuracy vs. Number of Trees plot]\n",
        "\n",
        "\n",
        "# --- 7. Visualization: Feature Importance (Part B, Task 3) ---\n",
        "\n",
        "importance_scores = rf_model.feature_importances_\n",
        "# Sort features by importance\n",
        "sorted_indices = np.argsort(importance_scores)[::-1]\n",
        "sorted_importances = importance_scores[sorted_indices]\n",
        "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.bar(sorted_feature_names, sorted_importances, color='teal')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Normalized Importance Score (Gini Reduction)')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show() #\n"
      ],
      "metadata": {
        "id": "41W5vyK3rcgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}